{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1d5ebed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.6.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Tensorflow version: \", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a37c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 12485830843488267260\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b268c2c5",
   "metadata": {},
   "source": [
    "### Tensor: Multi deimensional array used in Tensorflow\n",
    "* Constant\n",
    "* Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06b1f1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1661fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor a's rank is 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a scalar as constant\n",
    "a = tf.constant(3)\n",
    "print(\"Tensor a's rank is {}\".format(tf.rank(a)))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64fde118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor rank is 2\n"
     ]
    }
   ],
   "source": [
    "# define a matrix as constant\n",
    "b = tf.constant([[1, 2], [0, 0]])\n",
    "print(\"Tensor rank is {}\".format(tf.rank(b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec0bccbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor rank is 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float64, numpy=\n",
       "array([[0.2379978 , 0.602721  ],\n",
       "       [0.50759411, 0.77619511]])>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define ndarray as constant\n",
    "c = tf.constant(np.random.rand(2,2))\n",
    "print(\"Tensor rank is {}\".format(tf.rank(c)))\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9e02b65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define a scalar as variable\n",
    "x = tf.Variable(3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "916e4c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=3>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert a constant as a variable\n",
    "x = tf.Variable(a)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a38b422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'UnreadVariable' shape=() dtype=int32, numpy=0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Variable could assign new value, but constant could not be re-assigned\n",
    "# a.assign(0) # wrong\n",
    "x.assign(0) # right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cba59b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# type cast\n",
    "b = tf.constant([[1, 2], [3, 4]])\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2462236c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2.]\n",
      " [3. 4.]], shape=(2, 2), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "b = tf.cast(b, dtype = tf.float32)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78ebd930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27\n",
      "tf.Tensor(27, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# simple function\n",
    "# x = tf.Variable(2) # correct\n",
    "# y = tf.Variable(5)\n",
    "x = tf.constant(2) # correct\n",
    "y = tf.constant(5)\n",
    "f = x * x * y + y + 2\n",
    "print(f.numpy()) # f.numpy() show the value of numpy\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1898b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_test = tf.constant([[1,2],[3,4]])\n",
    "tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3830db0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([2, 2])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "797eaaad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_test.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "189d7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 7,  8],\n",
       "       [ 9, 10]], dtype=int32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# broadcasting: add the int to every element in the matrix\n",
    "tensor_test = tensor_test + 6\n",
    "tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c249825f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\n",
       "array([[ 1,  4],\n",
       "       [ 9, 16]], dtype=int32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# square\n",
    "tensor_test = tf.constant([[1,2],[3,4]])\n",
    "tensor_test = tf.square(tensor_test)\n",
    "tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c613849d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 2.],\n",
       "       [3., 4.]], dtype=float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sqrt, has to be float for sqrt\n",
    "tensor_test = tf.constant([[1., 4.], [9.,16.]])\n",
    "tensor_test = tf.sqrt(tensor_test)\n",
    "tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e5c6c12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[ 7., 10.],\n",
       "       [15., 22.]], dtype=float32)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_test = tf.matmul(tensor_test, tensor_test)\n",
    "tensor_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b294d51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,  16],\n",
       "       [ 81, 256]], dtype=int32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy function could be used for tensor calculation, return numpy array\n",
    "tensor_test = tf.constant([[1,4],[9,16]])\n",
    "np.square(tensor_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff0ecb6",
   "metadata": {},
   "source": [
    "### TensorFlow automatic derivation mechanism\n",
    "#### only calculate derivation for variable tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "42e87205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/df where f=(x^2):\n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tf.GradientTape --> derivation\n",
    "x = tf.Variable([2.]) # has to be variable, has to be float\n",
    "# x = tf.Variable([2])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    \n",
    "print(\"The gradient df/df where f=(x^2):\\n\", tape.gradient(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4313e038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/fx where f = (x ^ 2):\n",
      " None\n",
      "WARNING:tensorflow:The dtype of the target tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "WARNING:tensorflow:The dtype of the source tensor must be floating (e.g. tf.float32) when calling GradientTape.gradient, got tf.int32\n",
      "The gradient df/dx where f=(x ^ 2):\n",
      " None\n",
      "The gradient df/dx where f=(x ^ 2):\n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# the variable tensor has to be float, when calculate its derivation\n",
    "\n",
    "# wrong: has to be variable, not constant\n",
    "x = tf.constant([2.0])\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    \n",
    "print('The gradient df/fx where f = (x ^ 2):\\n', tape.gradient(f, x))\n",
    "\n",
    "# wrong: has to be float, not int\n",
    "x = tf.Variable([2])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    \n",
    "print('The gradient df/dx where f=(x ^ 2):\\n', tape.gradient(f, x))\n",
    "\n",
    "# correct\n",
    "x = tf.Variable([2.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    \n",
    "print('The gradient df/dx where f=(x ^ 2):\\n', tape.gradient(f, x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b252172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/dx where f = (x ^ 2):\n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n",
      "The gradient dh/dx where h = (x ^ 3):\n",
      " tf.Tensor([27.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# calculate gradient descent for multiple functions\n",
    "x = tf.Variable([2.0])\n",
    "y = tf.Variable([3.0])\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    h = y ** 3\n",
    "\n",
    "# Print gradient output\n",
    "print('The gradient df/dx where f = (x ^ 2):\\n', tape.gradient(f, x))\n",
    "print('The gradient dh/dx where h = (x ^ 3):\\n', tape.gradient(h, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cc93a3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/dx where f = (x ^ 2):\n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-13603044159c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Print gradient output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The gradient df/dx where f = (x ^ 2):\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The gradient dh/dy where h = (y ^ 3):\\n'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \"\"\"\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1032\u001b[0;31m       raise RuntimeError(\"A non-persistent GradientTape can only be used to \"\n\u001b[0m\u001b[1;32m   1033\u001b[0m                          \"compute one set of gradients (or jacobians)\")\n\u001b[1;32m   1034\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recording\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: A non-persistent GradientTape can only be used to compute one set of gradients (or jacobians)"
     ]
    }
   ],
   "source": [
    "# wrong: persistent=False: only get derivation for the 1st variable\n",
    "x = tf.Variable([2.0])\n",
    "y = tf.Variable([3.0])\n",
    "\n",
    "with tf.GradientTape(persistent=False, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    h = y ** 3\n",
    "\n",
    "# Print gradient output\n",
    "print('The gradient df/dx where f = (x ^ 2):\\n', tape.gradient(f, x))\n",
    "print('The gradient dh/dy where h = (y ^ 3):\\n', tape.gradient(h, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbf530ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/dx where f = (x ^ 2):\n",
      " tf.Tensor([4.], shape=(1,), dtype=float32)\n",
      "The gradient dh/dy where h = (y ^ 3):\n",
      " tf.Tensor([27.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# persistent=True: could get derivation for all variables\n",
    "# watch_accessed_variables=True: trace all vars\n",
    "x = tf.Variable([2.0])\n",
    "y = tf.Variable([3.0])\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2\n",
    "    h = y ** 3\n",
    "\n",
    "# Print gradient output\n",
    "print('The gradient df/dx where f = (x ^ 2):\\n', tape.gradient(f, x))\n",
    "print('The gradient dh/dy where h = (y ^ 3):\\n', tape.gradient(h, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d467505f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/dx where f = (x ^ 2):\n",
      " None\n",
      "The gradient dh/dy where h = (y ^ 3):\n",
      " tf.Tensor([27.], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# watch_accessed_variables=False: trace the var(s) that assigns to tape.watch()\n",
    "x = tf.Variable([2.0])\n",
    "y = tf.Variable([3.0])\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=False) as tape:\n",
    "    tape.watch(y)\n",
    "    f = x ** 2\n",
    "    h = y ** 3\n",
    "\n",
    "# Print gradient output\n",
    "print('The gradient df/dx where f = (x ^ 2):\\n', tape.gradient(f, x))\n",
    "print('The gradient dh/dy where h = (y ^ 3):\\n', tape.gradient(h, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b12e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient df/dx and df/dy where f = (x ^ 2 * y + y + 2):\n",
      " [<tf.Tensor: shape=(1,), dtype=float32, numpy=array([12.], dtype=float32)>, <tf.Tensor: shape=(1,), dtype=float32, numpy=array([5.], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "# calculate multi-var's derivation\n",
    "x = tf.Variable([2.0])\n",
    "y = tf.Variable([3.0])\n",
    "\n",
    "with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:\n",
    "    f = x ** 2 * y + y + 2\n",
    "\n",
    "# Print gradient output\n",
    "print('The gradient df/dx and df/dy where f = (x ^ 2 * y + y + 2):\\n', tape.gradient(f, [x, y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502e489e",
   "metadata": {},
   "source": [
    "### Linear Regression in Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b61cd6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error as mse\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c50125f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X):\n",
    "    \"\"\"\n",
    "    input: x\n",
    "    output: y = 0.7x - 42(56)\n",
    "    \"\"\"\n",
    "    return 0.7 * X - 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1c7aecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate data\n",
    "N = 100\n",
    "noise_level = 0.8\n",
    "trainX = np.linspace(165, 190, N)\n",
    "# np.random.shuffle(trainX)\n",
    "# print(trainX)\n",
    "trainY = f(trainX) + np.random.randn(N) * noise_level\n",
    "# print(trainY)\n",
    "learning_rate = 0.1\n",
    "training_epochs = 300\n",
    "display_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "20cd2ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd3a3d55df0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(trainX, trainY, label='Original data')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "02f6b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_forward(x, w, b):\n",
    "    return x * w + b\n",
    "\n",
    "# use MSE (mean square error)\n",
    "def loss_function(y, y_hat):\n",
    "    return tf.reduce_mean(tf.square(y - y_hat))\n",
    "\n",
    "def data_loader(x, y, batch_size, shuffle=True):\n",
    "    ind = list(range(len(x)))\n",
    "    if shuffle == True:\n",
    "        np.random.shuffle(ind)\n",
    "    for i in range(0, len(ind), batch_size):\n",
    "        idx = np.array(ind[i:min(i + batch_size, len(x))])\n",
    "        yield x[idx], y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1c030d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init param\n",
    "w = tf.Variable(np.random.randn())\n",
    "b = tf.Variable(np.random.randn())\n",
    "batch_size = 32\n",
    "\n",
    "trainX = np.reshape(trainX, (-1, 1))\n",
    "trainY = np.reshape(trainY, (-1, 1))\n",
    "trainX, testX, trainY, testY = train_test_split(trainX,\n",
    "                                                trainY,\n",
    "                                                test_size=0.2,\n",
    "                                                random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6f548ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "x_scaler = StandardScaler()\n",
    "y_scaler = StandardScaler()\n",
    "# trainX = (trainX - trainX.min()) / (trainX.max() - trainX.min())\n",
    "# trainY = (trainY - trainY.min()) / (trainY.max() - trainY.min())\n",
    "\n",
    "trainX = x_scaler.fit_transform(trainX)\n",
    "trainY = y_scaler.fit_transform(trainY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f67e031f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd3685db100>]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# observe init fit line\n",
    "plt.scatter(trainX, trainY, label='Original Data')\n",
    "plt.plot(trainX, lr_forward(trainX, w, b), c='y', label='Fitted Line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7c95fc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss 0.154033\n",
      "epoch 2, loss 0.054151\n",
      "epoch 3, loss 0.029142\n",
      "epoch 4, loss 0.022642\n",
      "epoch 5, loss 0.020482\n",
      "epoch 6, loss 0.019687\n",
      "epoch 7, loss 0.019447\n",
      "epoch 8, loss 0.019484\n",
      "epoch 9, loss 0.019495\n",
      "epoch 10, loss 0.019491\n",
      "epoch 11, loss 0.019466\n",
      "epoch 12, loss 0.019456\n",
      "epoch 13, loss 0.019550\n",
      "epoch 14, loss 0.019657\n",
      "epoch 15, loss 0.019529\n",
      "epoch 16, loss 0.019452\n",
      "epoch 17, loss 0.019458\n",
      "epoch 18, loss 0.019475\n",
      "epoch 19, loss 0.019490\n",
      "epoch 20, loss 0.019537\n",
      "epoch 21, loss 0.019475\n",
      "epoch 22, loss 0.019442\n",
      "epoch 23, loss 0.019444\n",
      "epoch 24, loss 0.019456\n",
      "epoch 25, loss 0.019473\n",
      "epoch 26, loss 0.019444\n",
      "epoch 27, loss 0.019445\n",
      "epoch 28, loss 0.019455\n",
      "epoch 29, loss 0.019530\n",
      "epoch 30, loss 0.019525\n",
      "epoch 31, loss 0.019463\n",
      "epoch 32, loss 0.019505\n",
      "epoch 33, loss 0.019490\n",
      "epoch 34, loss 0.019461\n",
      "epoch 35, loss 0.019470\n",
      "epoch 36, loss 0.019490\n",
      "epoch 37, loss 0.019441\n",
      "epoch 38, loss 0.019486\n",
      "epoch 39, loss 0.019534\n",
      "epoch 40, loss 0.019506\n",
      "epoch 41, loss 0.019544\n",
      "epoch 42, loss 0.019547\n",
      "epoch 43, loss 0.019463\n",
      "epoch 44, loss 0.019466\n",
      "epoch 45, loss 0.019545\n",
      "epoch 46, loss 0.019463\n",
      "epoch 47, loss 0.019468\n",
      "epoch 48, loss 0.019557\n",
      "epoch 49, loss 0.019442\n",
      "epoch 50, loss 0.019451\n",
      "epoch 51, loss 0.019502\n",
      "epoch 52, loss 0.019452\n",
      "epoch 53, loss 0.019579\n",
      "epoch 54, loss 0.019489\n",
      "epoch 55, loss 0.019454\n",
      "epoch 56, loss 0.019455\n",
      "epoch 57, loss 0.019442\n",
      "epoch 58, loss 0.019449\n",
      "epoch 59, loss 0.019482\n",
      "epoch 60, loss 0.019454\n",
      "epoch 61, loss 0.019453\n",
      "epoch 62, loss 0.019460\n",
      "epoch 63, loss 0.019469\n",
      "epoch 64, loss 0.019447\n",
      "epoch 65, loss 0.019486\n",
      "epoch 66, loss 0.019444\n",
      "epoch 67, loss 0.019458\n",
      "epoch 68, loss 0.019508\n",
      "epoch 69, loss 0.019607\n",
      "epoch 70, loss 0.019468\n",
      "epoch 71, loss 0.019444\n",
      "epoch 72, loss 0.019505\n",
      "epoch 73, loss 0.019521\n",
      "epoch 74, loss 0.019565\n",
      "epoch 75, loss 0.019536\n",
      "epoch 76, loss 0.019490\n",
      "epoch 77, loss 0.019454\n",
      "epoch 78, loss 0.019452\n",
      "epoch 79, loss 0.019453\n",
      "epoch 80, loss 0.019466\n",
      "epoch 81, loss 0.019446\n",
      "epoch 82, loss 0.019514\n",
      "epoch 83, loss 0.019476\n",
      "epoch 84, loss 0.019449\n",
      "epoch 85, loss 0.019470\n",
      "epoch 86, loss 0.019670\n",
      "epoch 87, loss 0.019460\n",
      "epoch 88, loss 0.019441\n",
      "epoch 89, loss 0.019462\n",
      "epoch 90, loss 0.019458\n",
      "epoch 91, loss 0.019502\n",
      "epoch 92, loss 0.019485\n",
      "epoch 93, loss 0.019484\n",
      "epoch 94, loss 0.019491\n",
      "epoch 95, loss 0.019628\n",
      "epoch 96, loss 0.019548\n",
      "epoch 97, loss 0.019566\n",
      "epoch 98, loss 0.019442\n",
      "epoch 99, loss 0.019497\n",
      "epoch 100, loss 0.019453\n",
      "epoch 101, loss 0.019452\n",
      "epoch 102, loss 0.019455\n",
      "epoch 103, loss 0.019457\n",
      "epoch 104, loss 0.019447\n",
      "epoch 105, loss 0.019459\n",
      "epoch 106, loss 0.019455\n",
      "epoch 107, loss 0.019454\n",
      "epoch 108, loss 0.019458\n",
      "epoch 109, loss 0.019509\n",
      "epoch 110, loss 0.019443\n",
      "epoch 111, loss 0.019506\n",
      "epoch 112, loss 0.019496\n",
      "epoch 113, loss 0.019468\n",
      "epoch 114, loss 0.019530\n",
      "epoch 115, loss 0.019445\n",
      "epoch 116, loss 0.019445\n",
      "epoch 117, loss 0.019490\n",
      "epoch 118, loss 0.019446\n",
      "epoch 119, loss 0.019455\n",
      "epoch 120, loss 0.019453\n",
      "epoch 121, loss 0.019462\n",
      "epoch 122, loss 0.019464\n",
      "epoch 123, loss 0.019500\n",
      "epoch 124, loss 0.019446\n",
      "epoch 125, loss 0.019448\n",
      "epoch 126, loss 0.019477\n",
      "epoch 127, loss 0.019443\n",
      "epoch 128, loss 0.019451\n",
      "epoch 129, loss 0.019452\n",
      "epoch 130, loss 0.019498\n",
      "epoch 131, loss 0.019472\n",
      "epoch 132, loss 0.019460\n",
      "epoch 133, loss 0.019485\n",
      "epoch 134, loss 0.019450\n",
      "epoch 135, loss 0.019471\n",
      "epoch 136, loss 0.019502\n",
      "epoch 137, loss 0.019507\n",
      "epoch 138, loss 0.019451\n",
      "epoch 139, loss 0.019454\n",
      "epoch 140, loss 0.019567\n",
      "epoch 141, loss 0.019545\n",
      "epoch 142, loss 0.019543\n",
      "epoch 143, loss 0.019578\n",
      "epoch 144, loss 0.019512\n",
      "epoch 145, loss 0.019447\n",
      "epoch 146, loss 0.019477\n",
      "epoch 147, loss 0.019448\n",
      "epoch 148, loss 0.019480\n",
      "epoch 149, loss 0.019473\n",
      "epoch 150, loss 0.019458\n",
      "epoch 151, loss 0.019591\n",
      "epoch 152, loss 0.019554\n",
      "epoch 153, loss 0.019582\n",
      "epoch 154, loss 0.019548\n",
      "epoch 155, loss 0.019488\n",
      "epoch 156, loss 0.019454\n",
      "epoch 157, loss 0.019452\n",
      "epoch 158, loss 0.019474\n",
      "epoch 159, loss 0.019520\n",
      "epoch 160, loss 0.019476\n",
      "epoch 161, loss 0.019520\n",
      "epoch 162, loss 0.019506\n",
      "epoch 163, loss 0.019450\n",
      "epoch 164, loss 0.019496\n",
      "epoch 165, loss 0.019465\n",
      "epoch 166, loss 0.019447\n",
      "epoch 167, loss 0.019449\n",
      "epoch 168, loss 0.019452\n",
      "epoch 169, loss 0.019451\n",
      "epoch 170, loss 0.019513\n",
      "epoch 171, loss 0.019544\n",
      "epoch 172, loss 0.019493\n",
      "epoch 173, loss 0.019472\n",
      "epoch 174, loss 0.019462\n",
      "epoch 175, loss 0.019498\n",
      "epoch 176, loss 0.019443\n",
      "epoch 177, loss 0.019510\n",
      "epoch 178, loss 0.019462\n",
      "epoch 179, loss 0.019540\n",
      "epoch 180, loss 0.019456\n",
      "epoch 181, loss 0.019448\n",
      "epoch 182, loss 0.019549\n",
      "epoch 183, loss 0.019460\n",
      "epoch 184, loss 0.019517\n",
      "epoch 185, loss 0.019443\n",
      "epoch 186, loss 0.019451\n",
      "epoch 187, loss 0.019445\n",
      "epoch 188, loss 0.019442\n",
      "epoch 189, loss 0.019444\n",
      "epoch 190, loss 0.019460\n",
      "epoch 191, loss 0.019458\n",
      "epoch 192, loss 0.019449\n",
      "epoch 193, loss 0.019489\n",
      "epoch 194, loss 0.019494\n",
      "epoch 195, loss 0.019474\n",
      "epoch 196, loss 0.019518\n",
      "epoch 197, loss 0.019463\n",
      "epoch 198, loss 0.019475\n",
      "epoch 199, loss 0.019534\n",
      "epoch 200, loss 0.019509\n",
      "epoch 201, loss 0.019463\n",
      "epoch 202, loss 0.019464\n",
      "epoch 203, loss 0.019551\n",
      "epoch 204, loss 0.019445\n",
      "epoch 205, loss 0.019474\n",
      "epoch 206, loss 0.019540\n",
      "epoch 207, loss 0.019615\n",
      "epoch 208, loss 0.019525\n",
      "epoch 209, loss 0.019523\n",
      "epoch 210, loss 0.019446\n",
      "epoch 211, loss 0.019468\n",
      "epoch 212, loss 0.019513\n",
      "epoch 213, loss 0.019457\n",
      "epoch 214, loss 0.019474\n",
      "epoch 215, loss 0.019444\n",
      "epoch 216, loss 0.019467\n",
      "epoch 217, loss 0.019453\n",
      "epoch 218, loss 0.019515\n",
      "epoch 219, loss 0.019515\n",
      "epoch 220, loss 0.019448\n",
      "epoch 221, loss 0.019446\n",
      "epoch 222, loss 0.019449\n",
      "epoch 223, loss 0.019446\n",
      "epoch 224, loss 0.019488\n",
      "epoch 225, loss 0.019467\n",
      "epoch 226, loss 0.019487\n",
      "epoch 227, loss 0.019471\n",
      "epoch 228, loss 0.019489\n",
      "epoch 229, loss 0.019475\n",
      "epoch 230, loss 0.019484\n",
      "epoch 231, loss 0.019442\n",
      "epoch 232, loss 0.019533\n",
      "epoch 233, loss 0.019498\n",
      "epoch 234, loss 0.019443\n",
      "epoch 235, loss 0.019570\n",
      "epoch 236, loss 0.019458\n",
      "epoch 237, loss 0.019444\n",
      "epoch 238, loss 0.019483\n",
      "epoch 239, loss 0.019553\n",
      "epoch 240, loss 0.019494\n",
      "epoch 241, loss 0.019480\n",
      "epoch 242, loss 0.019459\n",
      "epoch 243, loss 0.019469\n",
      "epoch 244, loss 0.019491\n",
      "epoch 245, loss 0.019456\n",
      "epoch 246, loss 0.019518\n",
      "epoch 247, loss 0.019454\n",
      "epoch 248, loss 0.019521\n",
      "epoch 249, loss 0.019532\n",
      "epoch 250, loss 0.019445\n",
      "epoch 251, loss 0.019482\n",
      "epoch 252, loss 0.019460\n",
      "epoch 253, loss 0.019498\n",
      "epoch 254, loss 0.019466\n",
      "epoch 255, loss 0.019478\n",
      "epoch 256, loss 0.019460\n",
      "epoch 257, loss 0.019450\n",
      "epoch 258, loss 0.019449\n",
      "epoch 259, loss 0.019451\n",
      "epoch 260, loss 0.019553\n",
      "epoch 261, loss 0.019442\n",
      "epoch 262, loss 0.019479\n",
      "epoch 263, loss 0.019453\n",
      "epoch 264, loss 0.019454\n",
      "epoch 265, loss 0.019456\n",
      "epoch 266, loss 0.019557\n",
      "epoch 267, loss 0.019443\n",
      "epoch 268, loss 0.019442\n",
      "epoch 269, loss 0.019445\n",
      "epoch 270, loss 0.019473\n",
      "epoch 271, loss 0.019451\n",
      "epoch 272, loss 0.019474\n",
      "epoch 273, loss 0.019471\n",
      "epoch 274, loss 0.019453\n",
      "epoch 275, loss 0.019564\n",
      "epoch 276, loss 0.019441\n",
      "epoch 277, loss 0.019474\n",
      "epoch 278, loss 0.019588\n",
      "epoch 279, loss 0.019548\n",
      "epoch 280, loss 0.019494\n",
      "epoch 281, loss 0.019541\n",
      "epoch 282, loss 0.019519\n",
      "epoch 283, loss 0.019519\n",
      "epoch 284, loss 0.019570\n",
      "epoch 285, loss 0.019442\n",
      "epoch 286, loss 0.019489\n",
      "epoch 287, loss 0.019502\n",
      "epoch 288, loss 0.019448\n",
      "epoch 289, loss 0.019470\n",
      "epoch 290, loss 0.019478\n",
      "epoch 291, loss 0.019474\n",
      "epoch 292, loss 0.019481\n",
      "epoch 293, loss 0.019456\n",
      "epoch 294, loss 0.019472\n",
      "epoch 295, loss 0.019469\n",
      "epoch 296, loss 0.019477\n",
      "epoch 297, loss 0.019470\n",
      "epoch 298, loss 0.019456\n",
      "epoch 299, loss 0.019467\n",
      "epoch 300, loss 0.019473\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "for i in range(training_epochs):\n",
    "    for X, y in data_loader(trainX, trainY, batch_size):\n",
    "        with tf.GradientTape(persistent=True, watch_accessed_variables=True) as tape:        \n",
    "            # calculate the loss in this iteration\n",
    "            loss = loss_function(y, lr_forward(X, w, b))\n",
    "            \n",
    "        # calculate gradient, update param\n",
    "        delta_w, delta_b = tape.gradient(loss, [w, b])\n",
    "        w.assign_sub(learning_rate * delta_w)\n",
    "        b.assign_sub(learning_rate * delta_b)\n",
    "    \n",
    "    # calculate the total loss of this iteration\n",
    "    train_loss = loss_function(trainY, lr_forward(trainX, w, b))\n",
    "    print('epoch %d, loss %f' % (i + 1, tf.reduce_mean(train_loss)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "890e2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_x = x_scaler.transform(np.reshape(testX, (-1, 1)))\n",
    "predicted_y = y_scaler.inverse_transform(lr_forward(processed_x, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "980deb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9813898422929066"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(testY, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4cd90bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd3a3edca00>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(trainX, trainY, label='Original Data')\n",
    "plt.plot(trainX, lr_forward(trainX, w, b), c='y', label='Fitted Line')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "df4c15b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.99099463>\n",
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=-0.0056371493>\n"
     ]
    }
   ],
   "source": [
    "print(w)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bddc06ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([0.71529025], shape=(1,), dtype=float32)\n",
      "tf.Tensor([-44.8295], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# restore w_true & b_true\n",
    "print(w * y_scaler.scale_ / x_scaler.scale_)\n",
    "print(- w * y_scaler.scale_ / x_scaler.scale_ * x_scaler.mean_ + y_scaler.scale_ * b + y_scaler.mean_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3720466",
   "metadata": {},
   "source": [
    "### Keras API version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb6713cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 1)                 2         \n",
      "=================================================================\n",
      "Total params: 2\n",
      "Trainable params: 2\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Use Keras to build the linear model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(1, input_dim=1))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c795b14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "8/8 [==============================] - 0s 814us/step - loss: 0.1814 - mse: 0.1814\n",
      "Epoch 2/300\n",
      "8/8 [==============================] - 0s 783us/step - loss: 0.1369 - mse: 0.1369\n",
      "Epoch 3/300\n",
      "8/8 [==============================] - 0s 807us/step - loss: 0.1044 - mse: 0.1044\n",
      "Epoch 4/300\n",
      "8/8 [==============================] - 0s 889us/step - loss: 0.0808 - mse: 0.0808\n",
      "Epoch 5/300\n",
      "8/8 [==============================] - 0s 910us/step - loss: 0.0640 - mse: 0.0640\n",
      "Epoch 6/300\n",
      "8/8 [==============================] - 0s 761us/step - loss: 0.0516 - mse: 0.0516\n",
      "Epoch 7/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0428 - mse: 0.0428\n",
      "Epoch 8/300\n",
      "8/8 [==============================] - 0s 789us/step - loss: 0.0362 - mse: 0.0362\n",
      "Epoch 9/300\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0317 - mse: 0.0317\n",
      "Epoch 10/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0283 - mse: 0.0283\n",
      "Epoch 11/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0259 - mse: 0.0259\n",
      "Epoch 12/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0242 - mse: 0.0242\n",
      "Epoch 13/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0228 - mse: 0.0228\n",
      "Epoch 14/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0219 - mse: 0.0219\n",
      "Epoch 15/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0212 - mse: 0.0212\n",
      "Epoch 16/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0208 - mse: 0.0208\n",
      "Epoch 17/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0204 - mse: 0.0204\n",
      "Epoch 18/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0202 - mse: 0.0202\n",
      "Epoch 19/300\n",
      "8/8 [==============================] - 0s 918us/step - loss: 0.0200 - mse: 0.0200\n",
      "Epoch 20/300\n",
      "8/8 [==============================] - 0s 913us/step - loss: 0.0198 - mse: 0.0198\n",
      "Epoch 21/300\n",
      "8/8 [==============================] - 0s 961us/step - loss: 0.0197 - mse: 0.0197\n",
      "Epoch 22/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0197 - mse: 0.0197\n",
      "Epoch 23/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0197 - mse: 0.0197\n",
      "Epoch 24/300\n",
      "8/8 [==============================] - 0s 906us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 25/300\n",
      "8/8 [==============================] - 0s 846us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 26/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 27/300\n",
      "8/8 [==============================] - 0s 947us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 28/300\n",
      "8/8 [==============================] - 0s 985us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 29/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 30/300\n",
      "8/8 [==============================] - 0s 4ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 31/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 32/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 33/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 34/300\n",
      "8/8 [==============================] - 0s 896us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 35/300\n",
      "8/8 [==============================] - 0s 857us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 36/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 37/300\n",
      "8/8 [==============================] - 0s 939us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 38/300\n",
      "8/8 [==============================] - 0s 924us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 39/300\n",
      "8/8 [==============================] - 0s 938us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 40/300\n",
      "8/8 [==============================] - 0s 941us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 41/300\n",
      "8/8 [==============================] - 0s 871us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 42/300\n",
      "8/8 [==============================] - 0s 875us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 43/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 44/300\n",
      "8/8 [==============================] - 0s 785us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 45/300\n",
      "8/8 [==============================] - 0s 784us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 46/300\n",
      "8/8 [==============================] - 0s 747us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 47/300\n",
      "8/8 [==============================] - 0s 903us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 48/300\n",
      "8/8 [==============================] - 0s 774us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 49/300\n",
      "8/8 [==============================] - 0s 987us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 50/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 51/300\n",
      "8/8 [==============================] - 0s 935us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 52/300\n",
      "8/8 [==============================] - 0s 791us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 53/300\n",
      "8/8 [==============================] - 0s 999us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 54/300\n",
      "8/8 [==============================] - 0s 891us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 55/300\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 56/300\n",
      "8/8 [==============================] - 0s 984us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 57/300\n",
      "8/8 [==============================] - 0s 826us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 58/300\n",
      "8/8 [==============================] - 0s 783us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 59/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 60/300\n",
      "8/8 [==============================] - 0s 993us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 61/300\n",
      "8/8 [==============================] - 0s 924us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 62/300\n",
      "8/8 [==============================] - 0s 980us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 63/300\n",
      "8/8 [==============================] - 0s 901us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 64/300\n",
      "8/8 [==============================] - 0s 877us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 65/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 66/300\n",
      "8/8 [==============================] - 0s 930us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 67/300\n",
      "8/8 [==============================] - 0s 920us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 68/300\n",
      "8/8 [==============================] - 0s 885us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 69/300\n",
      "8/8 [==============================] - 0s 849us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 70/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 71/300\n",
      "8/8 [==============================] - 0s 907us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 72/300\n",
      "8/8 [==============================] - 0s 890us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 73/300\n",
      "8/8 [==============================] - 0s 916us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 74/300\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 75/300\n",
      "8/8 [==============================] - 0s 872us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 76/300\n",
      "8/8 [==============================] - 0s 880us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 77/300\n",
      "8/8 [==============================] - 0s 825us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 78/300\n",
      "8/8 [==============================] - 0s 856us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 79/300\n",
      "8/8 [==============================] - 0s 910us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 80/300\n",
      "8/8 [==============================] - 0s 752us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 81/300\n",
      "8/8 [==============================] - 0s 931us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 82/300\n",
      "8/8 [==============================] - 0s 892us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 83/300\n",
      "8/8 [==============================] - 0s 893us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 84/300\n",
      "8/8 [==============================] - 0s 834us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 85/300\n",
      "8/8 [==============================] - 0s 850us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 86/300\n",
      "8/8 [==============================] - 0s 819us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 87/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 939us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 88/300\n",
      "8/8 [==============================] - 0s 899us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 89/300\n",
      "8/8 [==============================] - 0s 786us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 90/300\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 91/300\n",
      "8/8 [==============================] - 0s 841us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 92/300\n",
      "8/8 [==============================] - 0s 871us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 93/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 94/300\n",
      "8/8 [==============================] - 0s 784us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 95/300\n",
      "8/8 [==============================] - 0s 798us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 96/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 97/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 98/300\n",
      "8/8 [==============================] - 0s 937us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 99/300\n",
      "8/8 [==============================] - 0s 973us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 100/300\n",
      "8/8 [==============================] - 0s 942us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 101/300\n",
      "8/8 [==============================] - 0s 886us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 102/300\n",
      "8/8 [==============================] - 0s 865us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 103/300\n",
      "8/8 [==============================] - 0s 889us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 104/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 105/300\n",
      "8/8 [==============================] - 0s 889us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 106/300\n",
      "8/8 [==============================] - 0s 913us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 107/300\n",
      "8/8 [==============================] - 0s 914us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 108/300\n",
      "8/8 [==============================] - 0s 901us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 109/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 110/300\n",
      "8/8 [==============================] - 0s 824us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 111/300\n",
      "8/8 [==============================] - 0s 782us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 112/300\n",
      "8/8 [==============================] - 0s 827us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 113/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 114/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 115/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 116/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 117/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 118/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 119/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 120/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 121/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 122/300\n",
      "8/8 [==============================] - 0s 936us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 123/300\n",
      "8/8 [==============================] - 0s 982us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 124/300\n",
      "8/8 [==============================] - 0s 795us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 125/300\n",
      "8/8 [==============================] - 0s 855us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 126/300\n",
      "8/8 [==============================] - 0s 893us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 127/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 128/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 129/300\n",
      "8/8 [==============================] - 0s 842us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 130/300\n",
      "8/8 [==============================] - 0s 965us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 131/300\n",
      "8/8 [==============================] - 0s 995us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 132/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 133/300\n",
      "8/8 [==============================] - 0s 907us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 134/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 135/300\n",
      "8/8 [==============================] - 0s 872us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 136/300\n",
      "8/8 [==============================] - 0s 938us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 137/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 138/300\n",
      "8/8 [==============================] - 0s 955us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 139/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 140/300\n",
      "8/8 [==============================] - 0s 851us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 141/300\n",
      "8/8 [==============================] - 0s 811us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 142/300\n",
      "8/8 [==============================] - 0s 799us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 143/300\n",
      "8/8 [==============================] - 0s 800us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 144/300\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 145/300\n",
      "8/8 [==============================] - 0s 873us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 146/300\n",
      "8/8 [==============================] - 0s 762us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 147/300\n",
      "8/8 [==============================] - 0s 830us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 148/300\n",
      "8/8 [==============================] - 0s 800us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 149/300\n",
      "8/8 [==============================] - 0s 915us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 150/300\n",
      "8/8 [==============================] - 0s 765us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 151/300\n",
      "8/8 [==============================] - 0s 845us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 152/300\n",
      "8/8 [==============================] - 0s 826us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 153/300\n",
      "8/8 [==============================] - 0s 772us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 154/300\n",
      "8/8 [==============================] - 0s 924us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 155/300\n",
      "8/8 [==============================] - 0s 814us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 156/300\n",
      "8/8 [==============================] - 0s 841us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 157/300\n",
      "8/8 [==============================] - 0s 821us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 158/300\n",
      "8/8 [==============================] - 0s 767us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 159/300\n",
      "8/8 [==============================] - 0s 843us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 160/300\n",
      "8/8 [==============================] - 0s 760us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 161/300\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 162/300\n",
      "8/8 [==============================] - 0s 864us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 163/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 164/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 165/300\n",
      "8/8 [==============================] - 0s 866us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 166/300\n",
      "8/8 [==============================] - 0s 863us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 167/300\n",
      "8/8 [==============================] - 0s 953us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 168/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 169/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 170/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 171/300\n",
      "8/8 [==============================] - 0s 957us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 172/300\n",
      "8/8 [==============================] - 0s 840us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 173/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 917us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 174/300\n",
      "8/8 [==============================] - 0s 963us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 175/300\n",
      "8/8 [==============================] - 0s 852us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 176/300\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 177/300\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 178/300\n",
      "8/8 [==============================] - 0s 938us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 179/300\n",
      "8/8 [==============================] - 0s 974us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 180/300\n",
      "8/8 [==============================] - 0s 791us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 181/300\n",
      "8/8 [==============================] - 0s 781us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 182/300\n",
      "8/8 [==============================] - 0s 869us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 183/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 184/300\n",
      "8/8 [==============================] - 0s 961us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 185/300\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 186/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 187/300\n",
      "8/8 [==============================] - 0s 819us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 188/300\n",
      "8/8 [==============================] - 0s 829us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 189/300\n",
      "8/8 [==============================] - 0s 839us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 190/300\n",
      "8/8 [==============================] - 0s 938us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 191/300\n",
      "8/8 [==============================] - 0s 994us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 192/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 193/300\n",
      "8/8 [==============================] - 0s 987us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 194/300\n",
      "8/8 [==============================] - 0s 849us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 195/300\n",
      "8/8 [==============================] - 0s 792us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 196/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 197/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 198/300\n",
      "8/8 [==============================] - 0s 925us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 199/300\n",
      "8/8 [==============================] - 0s 899us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 200/300\n",
      "8/8 [==============================] - 0s 788us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 201/300\n",
      "8/8 [==============================] - 0s 943us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 202/300\n",
      "8/8 [==============================] - 0s 979us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 203/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 204/300\n",
      "8/8 [==============================] - 0s 976us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 205/300\n",
      "8/8 [==============================] - 0s 774us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 206/300\n",
      "8/8 [==============================] - 0s 774us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 207/300\n",
      "8/8 [==============================] - 0s 890us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 208/300\n",
      "8/8 [==============================] - 0s 958us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 209/300\n",
      "8/8 [==============================] - 0s 811us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 210/300\n",
      "8/8 [==============================] - 0s 810us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 211/300\n",
      "8/8 [==============================] - 0s 927us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 212/300\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 213/300\n",
      "8/8 [==============================] - 0s 944us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 214/300\n",
      "8/8 [==============================] - 0s 864us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 215/300\n",
      "8/8 [==============================] - 0s 928us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 216/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 217/300\n",
      "8/8 [==============================] - 0s 815us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 218/300\n",
      "8/8 [==============================] - 0s 944us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 219/300\n",
      "8/8 [==============================] - 0s 777us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 220/300\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 221/300\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 222/300\n",
      "8/8 [==============================] - 0s 801us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 223/300\n",
      "8/8 [==============================] - 0s 982us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 224/300\n",
      "8/8 [==============================] - 0s 837us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 225/300\n",
      "8/8 [==============================] - 0s 757us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 226/300\n",
      "8/8 [==============================] - 0s 932us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 227/300\n",
      "8/8 [==============================] - 0s 811us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 228/300\n",
      "8/8 [==============================] - 0s 964us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 229/300\n",
      "8/8 [==============================] - 0s 789us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 230/300\n",
      "8/8 [==============================] - 0s 902us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 231/300\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 232/300\n",
      "8/8 [==============================] - 0s 943us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 233/300\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 234/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 235/300\n",
      "8/8 [==============================] - 0s 878us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 236/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 237/300\n",
      "8/8 [==============================] - 0s 829us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 238/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 239/300\n",
      "8/8 [==============================] - 0s 7ms/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 240/300\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 241/300\n",
      "8/8 [==============================] - 0s 933us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 242/300\n",
      "8/8 [==============================] - 0s 979us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 243/300\n",
      "8/8 [==============================] - 0s 763us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 244/300\n",
      "8/8 [==============================] - 0s 953us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 245/300\n",
      "8/8 [==============================] - 0s 749us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 246/300\n",
      "8/8 [==============================] - 0s 876us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 247/300\n",
      "8/8 [==============================] - 0s 720us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 248/300\n",
      "8/8 [==============================] - 0s 965us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 249/300\n",
      "8/8 [==============================] - 0s 766us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 250/300\n",
      "8/8 [==============================] - 0s 952us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 251/300\n",
      "8/8 [==============================] - 0s 768us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 252/300\n",
      "8/8 [==============================] - 0s 875us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 253/300\n",
      "8/8 [==============================] - 0s 755us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 254/300\n",
      "8/8 [==============================] - 0s 928us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 255/300\n",
      "8/8 [==============================] - 0s 712us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 256/300\n",
      "8/8 [==============================] - 0s 906us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 257/300\n",
      "8/8 [==============================] - 0s 806us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 258/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/8 [==============================] - 0s 985us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 259/300\n",
      "8/8 [==============================] - 0s 758us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 260/300\n",
      "8/8 [==============================] - 0s 919us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 261/300\n",
      "8/8 [==============================] - 0s 774us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 262/300\n",
      "8/8 [==============================] - 0s 957us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 263/300\n",
      "8/8 [==============================] - 0s 771us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 264/300\n",
      "8/8 [==============================] - 0s 816us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 265/300\n",
      "8/8 [==============================] - 0s 745us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 266/300\n",
      "8/8 [==============================] - 0s 830us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 267/300\n",
      "8/8 [==============================] - 0s 751us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 268/300\n",
      "8/8 [==============================] - 0s 753us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 269/300\n",
      "8/8 [==============================] - 0s 762us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 270/300\n",
      "8/8 [==============================] - 0s 780us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 271/300\n",
      "8/8 [==============================] - 0s 697us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 272/300\n",
      "8/8 [==============================] - 0s 838us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 273/300\n",
      "8/8 [==============================] - 0s 729us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 274/300\n",
      "8/8 [==============================] - 0s 730us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 275/300\n",
      "8/8 [==============================] - 0s 761us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 276/300\n",
      "8/8 [==============================] - 0s 812us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 277/300\n",
      "8/8 [==============================] - 0s 739us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 278/300\n",
      "8/8 [==============================] - 0s 781us/step - loss: 0.0196 - mse: 0.0196\n",
      "Epoch 279/300\n",
      "8/8 [==============================] - 0s 740us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 280/300\n",
      "8/8 [==============================] - 0s 841us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 281/300\n",
      "8/8 [==============================] - 0s 734us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 282/300\n",
      "8/8 [==============================] - 0s 998us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 283/300\n",
      "8/8 [==============================] - 0s 798us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 284/300\n",
      "8/8 [==============================] - 0s 952us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 285/300\n",
      "8/8 [==============================] - 0s 792us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 286/300\n",
      "8/8 [==============================] - 0s 856us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 287/300\n",
      "8/8 [==============================] - 0s 741us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 288/300\n",
      "8/8 [==============================] - 0s 977us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 289/300\n",
      "8/8 [==============================] - 0s 750us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 290/300\n",
      "8/8 [==============================] - 0s 945us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 291/300\n",
      "8/8 [==============================] - 0s 756us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 292/300\n",
      "8/8 [==============================] - 0s 845us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 293/300\n",
      "8/8 [==============================] - 0s 725us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 294/300\n",
      "8/8 [==============================] - 0s 884us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 295/300\n",
      "8/8 [==============================] - 0s 715us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 296/300\n",
      "8/8 [==============================] - 0s 833us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 297/300\n",
      "8/8 [==============================] - 0s 810us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 298/300\n",
      "8/8 [==============================] - 0s 1ms/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 299/300\n",
      "8/8 [==============================] - 0s 706us/step - loss: 0.0195 - mse: 0.0195\n",
      "Epoch 300/300\n",
      "8/8 [==============================] - 0s 793us/step - loss: 0.0195 - mse: 0.0195\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='sgd', loss='mse', metrics='mse')\n",
    "history = model.fit(trainX, trainY, steps_per_epoch=8, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5a509651",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_x = x_scaler.transform(np.reshape(testX, (-1, 1)))\n",
    "predicted_y = y_scaler.inverse_transform(model(processed_x.astype(\"float32\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "da043fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9811968851710628"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2_score(testY, predicted_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0d97aa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense/kernel:0' shape=(1, 1) dtype=float32, numpy=array([[0.9898361]], dtype=float32)>,\n",
       " <tf.Variable 'dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.00022722], dtype=float32)>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check params\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "05d1246c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.9898361]], dtype=float32), array([0.00022722], dtype=float32)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287e012d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
